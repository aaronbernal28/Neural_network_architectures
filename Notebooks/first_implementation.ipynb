{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSVnOHBy2nv"
      },
      "source": [
        "## Ecuaciones Maestras\n",
        "\n",
        "Llegamos a las ecuaciones maestras que arman el *algoritmo de retropropagación*\n",
        "\\begin{align}\\tag{1}\n",
        "x_j^{(\\ell)} &= \\phi\\left(\\sum_{i=1}^{d^{(\\ell-1)}} w_{ij}^{(\\ell)} x_i^{(\\ell-1)} - b_j^{(\\ell)}\\right),\\quad \\ell=1,\\dots,L-1\\\\ \\tag{2}\n",
        "s_j^{(\\ell)} &= \\sum_{i=1}^{d^{(\\ell-1)}} w_{ij}^{(\\ell)} x_i^{(\\ell-1)} - b_j^{(\\ell)}\\\\ \\tag{3}\n",
        "y_j &= x_j^{(L)} = \\sum_{i=1}^{d^{(L-1)}} \\lambda_{ij} x_i^{(L-1)}\\\\ \\tag{4}\n",
        "\\delta_j^{(L)} &= \\frac{\\partial C}{\\partial y_j}\\\\ \\tag{5}\n",
        "\\delta_i^{(\\ell-1)} &= \\phi'(s_i^{(\\ell-1)})\\sum_{j=1}^{d^{(\\ell)}} w_{ij}^{(\\ell)} \\delta_j^{(\\ell)},\\quad \\text{(fórmula de retropropagación de deltas)}\\\\ \\tag{6}\n",
        "\\partial_{\\lambda_{ij}} C &= x_i^{(L-1)}\\delta_j^{(L)}\\\\ \\tag{7}\n",
        "\\partial_{w_{ij}^{(\\ell)}} C &= x_i^{(\\ell-1)} \\delta_j^{(\\ell)}\\\\ \\tag{8}\n",
        "\\partial_{b_j^{(\\ell)}} C &= -\\delta_j^{(\\ell)}.\n",
        "\\end{align}\n",
        "\n",
        "Las ecuaciones (1), (2) y (3) son el forward method. Es decir el cálculo de la salida de la red.\n",
        "\n",
        "Las ecuaciones (4) y (5) son el algoritmo de retropropagación para el cálculo de deltas.\n",
        "\n",
        "Finalmente, las ecuaciones (6), (7) y (8) son el cálculo del gradiente de la función de costo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case 1:\n",
            "W: {(1, 0, 1): 1, (1, 1, 1): 1, (1, 2, 1): 1, (1, 3, 1): 1, (1, 0, 2): 1, (1, 1, 2): 1, (1, 2, 2): 1, (1, 3, 2): 1, (2, 0, 1): 1, (2, 1, 1): 1, (2, 2, 1): 1, (2, 0, 2): 1, (2, 1, 2): 1, (2, 2, 2): 1, (2, 0, 3): 1, (2, 1, 3): 1, (2, 2, 3): 1, (2, 0, 4): 1, (2, 1, 4): 1, (2, 2, 4): 1}\n",
            "B: {(1, 1): 0, (1, 2): 0, (2, 1): 0, (2, 2): 0, (2, 3): 0, (2, 4): 0}\n"
          ]
        }
      ],
      "source": [
        "def initialize_parameters(d):\n",
        "    '''\n",
        "    inputs:\n",
        "    d : list of L+1 integers where d[i] is the number of neurons in layer i\n",
        "        d[0] : number of featuress\n",
        "        d[L] : number of output neurons\n",
        "    N : number of samples in the training set\n",
        "\n",
        "    outputs:\n",
        "    W : list of L matrices where W[l, i, j] is the weight from input i to the neuron j in the layer l\n",
        "    B : list of L vectors where B[l, j] is the bias from the neuron j in the layer l\n",
        "    d_cost_W :\n",
        "    d_cost_B :\n",
        "\n",
        "    '''\n",
        "    L = len(d) - 1\n",
        "    W = {}\n",
        "    B = {}\n",
        "\n",
        "    for l in range(1, L+1):\n",
        "        for j in range(1, d[l]+1):\n",
        "            # Biases\n",
        "            B[l, j] = 0\n",
        "\n",
        "            # Weights\n",
        "            for i in range(d[l-1]+1):\n",
        "                W[l, i, j] = 1\n",
        "    return W, B\n",
        "\n",
        "# Test\n",
        "d_test = [3, 2, 4]\n",
        "W_test, B_test = initialize_parameters(d_test)\n",
        "print(\"Test Case 1:\")\n",
        "print(\"W:\", W_test)\n",
        "print(\"B:\", B_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_gradient(d):\n",
        "    L = len(d) - 1\n",
        "    d_cost_W_mean = {}\n",
        "    d_cost_B_mean = {}\n",
        "\n",
        "    for l in range(1, L+1):\n",
        "        for j in range(1, d[l]+1):\n",
        "            # gradient_W\n",
        "            d_cost_B_mean[l, j] = 0\n",
        "        \n",
        "            # gradient_B\n",
        "            for i in range(d[l-1]+1):\n",
        "                d_cost_W_mean[l, i, j] = 0\n",
        "    return d_cost_W_mean, d_cost_B_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X(0, 1) = 1.0\n",
            "X(0, 2) = -2.0\n",
            "X(1, 1) = 1.0\n",
            "X(1, 2) = 0.8\n",
            "X(2, 1) = 0.33999999999999997\n"
          ]
        }
      ],
      "source": [
        "def forward_propagation(x_0, W, B, phi, d):\n",
        "    assert len(x_0) == d[0], \"x_0 must have the same number of features as d[0]\"\n",
        "    L = len(d) - 1\n",
        "    X = {}\n",
        "    # copy x_0\n",
        "    for j in range(d[0]):\n",
        "        X[0, j + 1] = x_0[j]\n",
        "\n",
        "    # loop layers\n",
        "    for l in range(1, L + 1):\n",
        "        for j in range(1, d[l] + 1):\n",
        "            X[l, j] = - B[l, j]\n",
        "            for i in range(1, d[l - 1] + 1):\n",
        "                X[l, j] += W[l, i, j] * X[l - 1, i]\n",
        "            \n",
        "            # apply activation function\n",
        "            X[l, j] = phi(X[l, j])\n",
        "    return X\n",
        "\n",
        "# Define activation function (ReLU)\n",
        "def relu(x):\n",
        "    return max(0, x)\n",
        "\n",
        "# Input vector (x_0): 2 features\n",
        "x_0 = [1.0, -2.0]\n",
        "\n",
        "# Network architecture: 2 → 2 → 1\n",
        "d = [2, 2, 1]  # 2 input, 2 hidden, 1 output\n",
        "\n",
        "# Weights: W[l, i, j] (from neuron i in layer l-1 to neuron j in layer l)\n",
        "W = {\n",
        "    (1, 1, 1): 0.5, (1, 2, 1): -0.3,\n",
        "    (1, 1, 2): 0.8, (1, 2, 2): 0.1,\n",
        "    (2, 1, 1): 1.2, (2, 2, 1): -0.7\n",
        "}\n",
        "\n",
        "# Biases: B[l, j] (not nested, direct access by layer and neuron)\n",
        "B = {\n",
        "    (1, 1): 0.1, (1, 2): -0.2,\n",
        "    (2, 1): 0.3\n",
        "}\n",
        "\n",
        "# Call the function\n",
        "result = forward_propagation(x_0, W, B, relu, d)\n",
        "\n",
        "# Print result\n",
        "for key in sorted(result.keys()):\n",
        "    print(f\"X{key} = {result[key]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passed, deltas= {(2, 1): 1.0, (1, 1): 0.0, (1, 2): 1.0}\n"
          ]
        }
      ],
      "source": [
        "def backpropagation(X, y_0, W, B, d_phi, d):\n",
        "    delta = {}\n",
        "    L = len(d) - 1\n",
        "\n",
        "    for j in range(1, d[L] + 1):\n",
        "        # 6.2.25\n",
        "        s = - B[L, j] # (s_j)^L\n",
        "        for i in range(1, d[L - 1] + 1):\n",
        "            s += W[L, i, j] * X[L - 1, i]\n",
        "        delta[L, j] = (X[L, j] - y_0[j-1]) * d_phi(s) # squared error\n",
        "\n",
        "    for l in range(L-1, 0, -1):\n",
        "        for i in range(1, d[l] + 1):\n",
        "            # 6.2.26\n",
        "            s1 = - B[l, i] # (s_i)^{l-1}\n",
        "            for j in range(1, d[l - 1] + 1):\n",
        "                s1 += W[l, j, i] * X[l - 1, j]\n",
        "\n",
        "            s2 = 0\n",
        "            for j in range(1, d[l+1] + 1):\n",
        "                s2 += W[l + 1, i, j] * delta[l + 1, j]\n",
        "\n",
        "            delta[l, i] = d_phi(s1) * s2\n",
        "\n",
        "    return delta\n",
        "\n",
        "def d_relu(x):\n",
        "    return 1.0 if x > 0 else 0.0\n",
        "\n",
        "# ---------- Test Case 2: simple 2-2-1 network with ReLU ----------\n",
        "d2 = [2, 2, 1]\n",
        "X2 = {(0,1):1.0,(0,2):-2.0,(1,1):0.0,(1,2):2.0,(2,1):2.0}\n",
        "y2 = [1.0]\n",
        "W2 = {\n",
        "    (1,1,1):1.0,(1,2,1):1.0,(1,1,2):2.0,(1,2,2):0.0,\n",
        "    (2,1,1):1.0,(2,2,1):1.0\n",
        "}\n",
        "B2 = {(1,1):0.0,(1,2):0.0,(2,1):0.0}\n",
        "e2 = {(2,1):1.0,(1,1):0.0,(1,2):1.0}\n",
        "r2 = backpropagation(X2, y2, W2, B2, d_relu, d2)\n",
        "assert math.isclose(r2[(2,1)], e2[(2,1)], abs_tol=1e-9)\n",
        "assert math.isclose(r2[(1,1)], e2[(1,1)], abs_tol=1e-9)\n",
        "assert math.isclose(r2[(1,2)], e2[(1,2)], abs_tol=1e-9)\n",
        "print(\"Passed, deltas=\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def neural_network_regression(X_train: np.ndarray, y_train: np.ndarray,\n",
        "                              d: list, nabla: float, phi, d_phi, max_iter=10, eps=1e-3) -> {dict, dict}:\n",
        "    '''\n",
        "    inputs:\n",
        "\n",
        "    d : list of L+1 integers where d[i] is the number of neurons in layer i\n",
        "        d[0] : number of featuress\n",
        "        d[L] : number of output neurons\n",
        "    X_train: training data with shape (n_samples, d[0])\n",
        "    y_train: training labels with shape (n_samples, d[L])\n",
        "\n",
        "    nabla : learning rate > 0\n",
        "    phi : activation function to apply to the output layer\n",
        "        e.g. sigmoid, relu, softmax, etc.\n",
        "\n",
        "    d_phi : derivative of the activation function\n",
        "\n",
        "    outputs:\n",
        "    W : dict where W[l, i, j] is the weight from input i to the neuron j in the layer l\n",
        "    B : dict where B[l, j] is the bias from the neuron j in the layer l\n",
        "    '''\n",
        "    N = len(X_train)\n",
        "    W, B = initialize_parameters(d)\n",
        "    L = len(d) - 1\n",
        "\n",
        "    # loop epocas:\n",
        "    n = 0\n",
        "    grad_norm = 1\n",
        "    while n < max_iter and grad_norm > eps:\n",
        "        # initialize gradients\n",
        "        d_cost_W_mean, d_cost_B_mean = initialize_gradient(d)\n",
        "\n",
        "        # batch training\n",
        "        for k in range(N):\n",
        "            x_0 = X_train[k]\n",
        "            y_0 = y_train[k]\n",
        "\n",
        "            X = forward_propagation(x_0, W, B, phi, d)\n",
        "            delta = backpropagation(X, y_0, W, B, d_phi, d)\n",
        "\n",
        "            # acumulate gradients \n",
        "            for l in range(1, L+1):\n",
        "                for j in range(1, d[l]+1):\n",
        "                    # gradient_W\n",
        "                    d_cost_B_mean[l, j] -= delta[l][j]\n",
        "                    grad_norm += d_cost_B_mean[l, j]**2 \n",
        "\n",
        "                    # gradient_B\n",
        "                    for i in range(d[l-1]+1):\n",
        "                        d_cost_W_mean[l, i, j] += delta[l][j] * X[l-1][i]\n",
        "                        grad_norm += d_cost_W_mean[l, i, j]**2\n",
        "\n",
        "        # take the mean of the gradients and apply the gradient descent algorithm\n",
        "        for l in range(1, L+1):\n",
        "            for j in range(1, d[l]+1):\n",
        "                # gradient_B\n",
        "                d_cost_B_mean[k, l, j] /= N\n",
        "                B[l, j] += nabla * d_cost_B_mean[l, j]\n",
        "\n",
        "                for i in range(d[l-1]+1):\n",
        "                    # gradient_W\n",
        "                    d_cost_W_mean[k, l, i, j] /= N\n",
        "                    W[l, i, j] -= nabla * d_cost_W_mean[l, i, j]\n",
        "        grad_norm = np.sqrt(grad_norm)\n",
        "        n += 1\n",
        "    return W, B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5fq3BMTmeqt"
      },
      "source": [
        "## Forma matricial de las ecuaciones maestras\n",
        "\n",
        "Para la implementación es mejor escribir las ecuaciones (1)--(8) en forma matricial que es como trabaja el código.\n",
        "Los datos entonces se toman\n",
        "$$\n",
        "\\mathbf x^{(\\ell)} = (x_1^{(\\ell)},\\dots,x_{d^{(\\ell)}}^{(\\ell)})^T,\\ W^{(\\ell)} = (w_{ij}^{(\\ell)})_{i,j},\\ \\mathbf b^{(\\ell)} = (b_1^{(\\ell)},\\dots,b_{d^{(\\ell)}}^{(\\ell)})^T\n",
        "$$\n",
        "$$\n",
        "\\delta^{(\\ell)} = (\\delta_1^{(\\ell)},\\dots,\\delta_{d^{(\\ell)}}^{(\\ell)})^T,\\ \\mathbf s^{(\\ell)} = (s_1^{(\\ell)},\\dots,s_{d^{(\\ell)}}^{(\\ell)})^T\n",
        "$$\n",
        "con la convención de que aplicar la función de activación $\\phi$ sobre un vector es hacerlo componente a componente. Entonces las ecuaciones quedan\n",
        "\\begin{align}\n",
        "\\mathbf x^{(\\ell)} &= \\phi\\left(W^{(\\ell)^T} \\mathbf x^{(\\ell-1)} - \\mathbf b^{(\\ell)}\\right)\\\\\n",
        "\\delta^{(L)} &= \\nabla_{\\mathbf y}C(\\mathbf x^{(L)}, \\mathbf z) \\odot \\phi'(\\mathbf s^{(L)})\\\\\n",
        "\\delta^{(\\ell-1)} &= (W^{(\\ell)}\\delta^{(\\ell)})\\odot \\phi'(\\mathbf s^{(\\ell-1)})\\\\\n",
        "\\frac{\\partial C}{\\partial W^{(\\ell)}} &= \\mathbf x^{(\\ell-1)} \\delta^{(\\ell)^T}\\\\\n",
        "\\frac{\\partial C}{\\partial \\mathbf b^{(\\ell)}} &= -\\delta^{(\\ell)},\n",
        "\\end{align}\n",
        "donde $\\odot$ es el producto de Hadamard entre vectores que consiste en multiplicar componente a componente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
